% !TeX spellcheck = en_US
\begin{frame}
	
	\Subsection{Krylov Subspace Methods}
	\Subsubsection{Krylov Subspaces}
		\text{Situation:}
		\begin{itemize}
			\item $A\in GL_n(\mathbb{R})$ (invertible, $n$ typically large, but $A$ sparse)
			\item $b\in\mathbb{R}^n$
		\end{itemize}
	~\\
		We want to solve $Ax=b$, but we cannot work with the matrix as a (dense) ``array''. Instead we only have access to the mapping
		$$
		x\mapsto Ax~~~(\text{matrix-vector product}).
		$$
		Then given $b\in\mathbb{R}^n$, we can produce the vectors
		$$
		b,Ab,A^2b,\dots,A^kb.
		$$
		There is not much more to consider. Let us collect all linear combinations of these vectors and give it a name:\\
		\begin{definition}[Krylov$^*$ subspaces]
			Let $A\in\mathbb{R}^{n\times n}$ and $b\in\mathbb{R}^n$, then the set
			$$
			K_r(A,b):=~\text{span}\{b,Ab,A^2b,\dots,A^{r-1}b\}
			$$
			is called \textbf{Krylov Subspace} of order $r\geq 1$ generated by $A$ and $b$.
		\end{definition}
		~\\
		In order to develop an iterative scheme based on this definition, we look deeper into the Krylov subspaces and first collect some insightful observations.
		~\\
		~\\
		\footnotesize
		------------------------------------------\\
		\textit{$^*$named after the Russian engineer Alexei Krylov who developed the idea in a paper published around 1931.}
\end{frame}

\begin{frame}
	 \textbf{Remarks}
		\begin{itemize}
			\item [i)]
			If $b=0$, then $A^kb=0$ for all $A\in\mathbb{R}^{n\times n}$ and all $k\in\mathbb{N}$, so that
			$$
			K_r(A,0)=\{0\}~~~~~\forall ~A\in\mathbb{R}^{n\times n},~r\geq 1.
			$$
			\item [ii)]
			If $b\neq 0$ and $A=I$, then $A^kb=b$ for all $ k\in\mathbb{N}$, so that
$$
			 K_r(I,b)=~\text{span}\{b\}~~~~\forall ~b\neq 0,~r\geq 1.
$$
			\item [iii)]
			If $b$ is an eigenvector of $A$ to the eigenvalue $\lambda\in\sigma(A)$, then $A^kb=\lambda^kb$, so that $A^k b$ and $b$ are linearly dependent, implying
			$$
			K_r(A,b)=~\text{span}\{b\}~~~~\forall~~\text{eigenvectors~$b$~of}~A.
			$$
			\item [iv)]
			Insight from the power method:\\
			 Recall: Let $|\lambda_1|>|\lambda_2|\geq\dots|\lambda_n|$ and $b^Tv_1\neq 0$, then $\frac{A^kb}{\|A^kb\|}\rightarrow v_1$. \\
		 Thus, for large $k$ we have that the $A^kb$ point into a ``similar direction'' -- more precisely, into the direction of $v_1$. With other words the $A^kb$ become more and more linearly dependent.\\~\\
			\item [v)]
			Dimension of Krylov subspaces:\\
%			$\textcolor{cyan}{(\text{Recall: Definition Subspace})}$
			\begin{itemize}
				\item Since $(K_r(A,b)) \subset \Rn$ is spanned by $r$ vectors, we clearly have dim$(K_r(A,b))\leq \min(r,n)$.
				\item 
			 For $A\in GL_n(\mathbb{R})$ one can show that
				$$
				b,Ab,\dots,A^{r-1}b~\text{are independent}~~~~\forall~r\leq r_{\text{max}},
				$$
				where $r_{\text{max}}$ is the maximal dimension a Krylov subspace generated by $A$ and $b$ can have, i.e., $r_{\text{max}}:=\max_{s\leq n}(\text{dim}K_s(A,b))$.
			\end{itemize}
		\end{itemize}
\end{frame}

\begin{frame}
		\begin{itemize}

			\item [vi)] Next we state the crucial result, which forms the basis for the development of Krylov subspace methods:
				\begin{lemma}
				Let $A\in GL_n(\mathbb{R})$ and $b\in\mathbb{R}^n$, then there exists an order $r\leq n$ so that for the solution $x^*$ of $Ax=b$ we have
				$$
				x^* =A^{-1}b\in K_r(A,b).
				$$
				In particular, we find coefficients $\beta_0,\dots,\beta_{r-1}$, such that
				$$
				x^* =\sum_{j=0}^{r-1} \beta_j A^j b.
				$$
			\end{lemma}
			\begin{proof}
				\Hide{
					Consider the $(n+1)$ vectors $b,Ab,\dots,A^{n-1}b,A^n b$, which are necessarily dependent in $\mathbb{R}^n$. Thus due to the dependence we find $\alpha_0,\alpha_1,\dots,\alpha_n\in\mathbb{R}$, which are not all zero, so that
					\begin{align*}
					0&=\sum_{j=0}^n \alpha_j A^j b
					=\alpha_0 A^0b+\alpha_1 A^1b+\dots +\alpha_nA^nb\\
					&\stackrel{{\color{cyan}(k:=\text{smallest index with}~\alpha_k\neq 0)}}{=}\underbrace{\alpha_k}_{\neq 0}A^kb+\alpha_{k+1}A^{k+1}b+\dots+\alpha_nA^nb\\
					\stackrel{{\color{cyan}(A^{-(k+1)}\cdot |)}}{\Leftrightarrow} 0 &= \alpha_k\underbrace{A^{-1}b}_{=x^*}+\alpha_{k+1}A^0b+\dots+\alpha_n A^{n-k-1}b\\
					\stackrel{{\color{cyan}(\alpha_k\neq 0)}}{\Leftrightarrow} x^*&=\frac{\alpha_{k+1}}{-\alpha_k}A^0b+\frac{\alpha_{k+2}}{-\alpha_k}A^1b+\dots+\frac{\alpha_n}{-\alpha_k}A^{n-k-1}b=\sum_{j=0}^{r-1} \beta_j A^j b,
					\end{align*}
					with $\beta_j:=\frac{\alpha_{k+1+j}}{-\alpha_k}$ and $r:=n-k$.
				}
			\end{proof}
		\end{itemize}
\end{frame}



\begin{frame}
	\textbf{Idea of Krylov Subspace Methods}~\\~\\
	\Hide{
		\begin{itemize}
			\item In words, the latter lemma states that the sought--after solution can be found in a Krylov subspace of some order  $r\leq n$. However we neither generally know $r$ nor the linear coefficients $\beta_j$.
			\item This fact leads to the following general iterative scheme of so-called Krylov subspace methods:\\
{\color{header}	 In each iteration step $1\leq r\leq n$, minimize the residual $\|Ax-b\|_2$ over all $x\in K_r(A,b)$, i.e.,
			$$
			x_r:=\underset{x\in K_r(A,b)\subset\mathbb{R}^n}{\mathrm{argmin}}\|Ax-b\|^2_2.
			$$
			Thereby we generate a sequence $x_1,x_2,\dots,x_n$.}
			\item Since $r\leq n$,  we know that in exact arithmetic (neglecting presence of rounding errors) at latest the step $r=n~(x_n)$ gives us the exact solution, so that one may call these methods ``direct''. However $n$ is typically very large ($\geq 10^5$) and in many examples it turns out that a few steps may already give very good approximations.~\\~\\
			\item
			Such methods to solve $Ax=b$ have been developed in the early 1950s; the most famous are:
			\begin{itemize}
				\item 
				\textbf{Conjugate Gradient (CG)} (Hestenes, Stiefel; 1952) \\
				for $A$ symmetric and positive definite ~\\~\\
				\item
				\textbf{MINimal RESidual (MINRES)} (Paige, Saunders; 1975)\\
				 for $A$ symmetric and invertible  ~\\~\\
				\item
				\textbf{General Minimal RESidual (GMRES)} (Schultz, Saad; 1986)\\
				for $A\in GL_n(\mathbb{R})$ 
			\end{itemize}
		\end{itemize}
	}
$\to$ In this chapter we will derive the GMRES method.
\end{frame}

\begin{frame}
 \textbf{Comparison to other methods}\\~\\
	\begin{tabular}{l|l|l}
		Least squares&Krylov subspace&Splitting\\
		\hline
		&&~\\
		$\hat{x}:=\underset{x\in\mathbb{R}^n}{\mathrm{argmin}}\|Ax-b\|_2^2$&$x_r:=\underset{x\in K_r(A,b)}{\mathrm{argmin}}\|Ax-b\|_2^2$&$x_r:=Mx_{r-1}+Nb$\\
		$A\hat{x}=\underset{w\in\text{Im}(A)}{\mathrm{argmin}}\|w-b\|_2^2$&$Ax_r=\underset{w\in AK_r(A,b)}{\mathrm{argmin}}\|w-b\|_2^2$&~\\
		~&~&~\\
		$\rightsquigarrow$ Projection of $b$ onto&$\rightsquigarrow$ Projection of $b$ onto &~\\
		Im$(A)=A\mathbb{R}^n=\mathbb{R}^n$&$AK_r(A,b)=$ span$\{Ab,A^2b,\dots,A^rb\}$&~\\
		~&$\subseteq K_{r+1}(A,b)$&~\\
		~&~&~\\
	 Yields exact solution, i.e.,& In theory: Yields a finite sequence& Typically yields an infinite sequence\\
		$\hat{x}=A^{-1}b$& with $x_n=A^{-1}b$& with $x^r\xrightarrow{(k\to\infty)}A^{-1}b$
	\end{tabular}
\end{frame}

\begin{frame}
	As a preparation for what follows, we will first derive an iterative method to find orthonormal bases for Krylov subspaces:\\~\\
	\Subsubsection{The Arnoldi Iteration}
	
\begin{itemize}
	\item \textbf{Situation:} Let us consider the $r$-th Krylov subspace $$K_r(A,b)=\text{span}\{A^0b,A^1b,\ldots, A^{r-1}b\}$$ with $r\leq r_{\text{max}}=\max_{s\leq n}\left(~\text{dim}(K_s(A,b))\right)$, so that all $A^{j-1}b$ are independent and $\text{dim}(K_r(A,b))=r$.
%
\item\textbf{Aim:} Find an {orthonormal} basis $\{q_1,\ldots, q_r\}$ of $K_r(A,b)$.
%
\item\textbf{Idea:} Apply the Gram--Schmidt orthogonalization process to the linearly independent vectors $c_j:=A^{j-1}b,~1\leq j \leq r$.
\item\textbf{Recall Gram--Schmidt:} Let	$c_1,\dots,c_r\in\mathbb{R}^n$, where $~r\leq n$, be linearly independent vectors. Then an orthonormal basis $\{q_1,\dots,q_{r}\}$ of $\text{span}(c_1,\ldots, c_r)=\text{Im}(C)$ can be found by the following iterative scheme:
		\begin{align*}
		q_1&:=\frac{c_1}{\|c_1\|}\\
		\text{``substracting projections:''}~\widehat{q}_j&:= c_j-\sum_{\ell=1}^{j-1}q_\ell^\top c_j\cdot q_\ell,~~~r_{\ell j} := q_\ell^\top c_j\\
		\text{``normalization:''}~q_j&:=\frac{\widehat{q}_j}{\|\widehat{q}_j\|_2},~~~r_{jj}:=\|\widehat{q}_j\|_2
		\end{align*}
\end{itemize}
		\small
		The matrix perspective: Putting the vectors $q_j$ and $r_j$ (which are computed step by step) into matrices, say $Q$ and $R$, then we obtain the (reduced) $QR$-decomposition of $C$, i.e., a matrix $Q=[q_1,\ldots, q_r]\in\mathbb{R}^{n\times r}$ with orthonormal columns and an upper triangular matrix $R\in\mathbb{R}^{r\times r}$ with $r_{ii}\neq0$, so that $C=QR$, which implies (also see Lemma \ref{lem:kerImProducts}) $\text{Im}(C)=\text{Im}(QR)= \text{Im}(Q) =~\text{span}(q_1,\dots,q_r).$
\end{frame}

\begin{frame}
	Now we consider the specific choice $c_j:= A^{j-1}b$. By inserting these $c_j$ into the above scheme, we obtain the so called \textbf{\color{defgruen}Arnoldi iteration}:\\
	\begin{align*}
	q_1&:=\frac{b}{\|b\|_2}~~~~~{\small(\rightarrow~\text{orthonormal basis for}~K_1(A,b)=~\text{span}(b),~b\neq 0)}\\
	\textbf{\color{cyan}for}&~j=2,\ldots, r:\\
	\text{''substracting projections:''}~\widehat{q}_j&:=Aq_{j-1}-\sum_{\ell=1}^{j-1}q_\ell^\top(Aq_{j-1}) \cdot q_\ell,~~~~~h_{\ell,j-1}:= q_\ell^\top(Aq_{j-1})~~~\textbf{\color{orange}(*)} \\
	\text{''normalization:''}~q_j&:=\frac{\widehat{q}_j}{\|\widehat{q}_j\|_2},~~~~h_{j,j-1} :=\|\widehat{q}_j\|~~~\textbf{\color{orange}(**)} 
	\end{align*}
	~\\~\\
All in all, the Arnoldi process yields an orthonormal basis for $K_r(A,b)$, so that
	$$
	K_r(A,b)=~\text{span}(b,Ab,\dots,A^{r-1}b)=~\text{span}(q_1,\dots,q_r)
	$$
or with $Q_r := [q_1,\ldots, q_r] \in \R^{n\times r}$ in matrix form
$$K_r(A,b) = \text{Im}(Q_r)  .$$
\footnotesize
Remarks:
\begin{itemize}
	\item Rearranging \textbf{\color{orange}(*)} and \textbf{\color{orange}(**)} easily gives $$Aq_{j-1}={\color{cyan}\|\widehat{q}_j\|_2}\cdot q_j+\sum_{\ell=1}^{j-1}{\color{cyan}q_\ell^\top(Aq_{j-1})} \cdot q_\ell = \sum_{\ell=1}^j {\color{cyan}h_{\ell,j-1}}  q_\ell .$$ 
	With other words, $Aq_{j-1}$ is a linear combination of $q_1,\ldots, q_j$.
	\item Also observe for $j\leq r$: The first $q_1,\ldots, q_{j-1}$ are a basis for $K_{j-1}(A,b)$. Thus, assumed these are given, then in order to find an orthonormal basis for $K_{j}(A,b)$ we just need to compute one more vector, namely $q_j$. 
\end{itemize}
 
\end{frame}
 



\begin{frame}
	\Subsubsection{GMRES}
	\textit{\footnotesize(with Arnoldi and $x_0=0$)}\\
Let us recall the general idea of Krylov subspace methods: In each iteration step we compute
		$$
		x_r:=\underset{x\in K_r(A,b)}{\mathrm{argmin}}\|Ax-b\|^2_2.
		$$
~\\
Now we approach this minimization problem in a specific procedure resulting in the GMRES method:\\~\\
We first find an orthonormal basis $\{q_1,\dots,q_r\}$ of $K_r(A,b)$ so that $K_r(A,b)=~\text{span}(q_1,\dots,q_r)=~\text{Im}(Q_r)$, where $Q_r:=[q_1,\dots,q_r]\in\mathbb{R}^{n\times r}$. Then, since by definition $x_r \in K_r(A,b)$, the minimization problem can be rephrased as
$$
x_r=\underset{x\in~\text{Im}(Q_r)}{\mathrm{argmin}}\|Ax-b\|_2^2\stackrel{{\color{cyan}(x_r=Q_rc_r)}}{=}Q_r\cdot\underbrace{(\underset{c\in\mathbb{R}^r}{\mathrm{argmin}}\|{\color{orange}AQ_r}c-b\|_2^2)}_{{\color{cyan}=:c_r,~\text{standard least quares problem}}}.
$$
The least squares problem can then be solved with the help of the $QR$-decomposition of the design matrix ${\color{orange}AQ_r}$, say $\widetilde{Q}_r\widetilde{R}_r :={\color{orange}AQ_r}$, so that the corresponding normal equation reads as
$$
\widetilde{R}_rc_r=\widetilde{Q}_r^Tb~~~(\text{when is}~\widetilde{R}_r~\text{invertible?})
$$
Thus, GMRES boils down to the three steps. For $1 \leq r \leq n$, do
\begin{itemize} \color{header}
	\item \underline{Step 1:}
	Find an orthonormal basis for $K_r(A,b)$.
	\item \underline{Step 2:}
	Find $\widetilde{Q}_r\widetilde{R}_r:={\color{orange}AQ_r}$.
	\item \underline{Step 3:}
	Solve $
	\widetilde{R}_rc_r=\widetilde{Q}_r^Tb$ and set $x_r:=Q_rc_r$.
\end{itemize}
{Crucial:} We can iteratively compute step 1 and step 2, i.e.,  use $Q_{r-1},~\widetilde{R}_{r-1},~ \widetilde{Q}_{r-1}$ to obtain $Q_{r}, ~\widetilde{R}_r,~ \widetilde{Q}_{r}$!\\
\end{frame}

\begin{frame}
	\textbf{GMRES -- Step 1 }
``Find an orthonormal basis for $K_r(A,b)$''\\~\\
		Given $Q_{r-1}=(q_1,\dots,q_{r-1})\in\mathbb{R}^{n\times (r-1)}$ with Im$(Q_{r-1})=K_{r-1}(A,b)$ from the previous iteration step, let us compute
		$$
		q_r :=~\texttt{Arnoldi\_step}(q_1,\dots,q_{r-1};Aq_{r-1}).
		$$
(\textit{``orthogonalizing $Aq_{r-1}$ against all $q_1,\dots,q_{r-1}$ by subtracting projections and normalization''})
~\\~\\~\\
We recall the $r$-th Arnoldi iteration step in an algorithmic fashion:\\~\\

			Given $q_1(:=\frac{b}{\|b\|}),\dots,q_{r-1}$, then $q_r$ is computed by:

\begin{center}
				$q_r:=$\texttt{{\large \bf Arnoldi\_step}($q_1,\dots,q_{r-1}$; $v=Aq_{r-1}$):}\\[0.2cm]
			\hspace*{6cm}\begin{minipage}{0.8\textwidth}
							\For{$~\ell=1,\dots,r-1$}{
					$~~~~h_{\ell,r-1}=q_\ell^Tv$\\
					$~~~~~v=v-h_{\ell,r-1}q_\ell ~~~~\textit{\footnotesize (substracting projections)}$
				}
								$h_{r,r-1}=\|v\|$\\
				\If{$h_{r,r-1}\neq 0 $}{
			$q_r=\frac{v}{h_{r,r-1}} ~~~~~~~~~~~~\textit{\footnotesize (normalization)}$ \\
			\textbf{return} $q_r$ 
		}
		\textbf{return} $v$ 
			
			\end{minipage}
\end{center}
\end{frame}


\begin{frame}
In matrix notation this can be summed up as follows:
			\begin{align*}
			A\cdot\underbrace{\begin{pmatrix}
				|&~&|\\
				q_1&\cdots&q_{r-1}\\
				|&~&|
				\end{pmatrix}}_{{\color{cyan}=:Q_{r-1}\in\R^{n \times (r-1)}}}
			&=\underbrace{\begin{pmatrix}
				|&~&|\\
				Aq_1&\cdots&Aq_{r-1}\\
				|&~&|
				\end{pmatrix}}_{{\color{cyan}\text{\scriptsize Arnoldi:}~Aq_j=\sum_{\ell=1}^{j+1}h_{\ell,j}q_\ell}}
			=\underbrace{\begin{pmatrix}
				|&~&|&|\\
				q_1&\cdots&q_{r-1}&q_r\\
				|&~&|&|
				\end{pmatrix}}_{{\color{cyan}=:Q_r \in\R^{n \times r}}}
			\underbrace{\color{magenta}\begin{pmatrix}
				h_{1,1}&h_{1,2}&~&\cdots& \\
				h_{2,1}&h_{2,2}&~&~&\vdots\\
				0&h_{3,2}&~&~&~\\
				\vdots&0&\ddots&h_{r-1,r-2}&h_{r-1,r-1}\\
				{\color{orange}0}&{\color{orange}0}&{\color{orange}0}&{\color{orange}h_{r,r-2}}&{\color{orange}h_{r,r-1}}
				\end{pmatrix}}_{{\color{cyan}=:H_{r,r-1}\in\R^{r \times (r-1)}}}\\
			\stackrel{{\color{cyan}Q_{r-1}^T  \cdot|}}{\Leftrightarrow}
			~~~Q_{r-1}^TAQ_{r-1}
			&=Q_{r-1}^TQ_rH_{r,r-1}=
			\begin{pmatrix}
			~&|&0\\
			I_{(r-1)\times(r-1)}&|&\vdots\\
			~&|&0
			\end{pmatrix}
			\begin{pmatrix}
			{\color{magenta}H_{r-1}}\\---\\{\color{orange}\text{row}~r~\text{of}~H_{r,r-1}}
			\end{pmatrix} = {\color{magenta}H_{r-1}}
			\end{align*}
\begin{itemize}
	\item 			We observe that ${\color{magenta}H_{r-1}}\in\mathbb{R}^{(r-1)\times(r-1)}$ has only one subdiagonal. Such matrices are called (upper) \textbf{\color{defgruen}Hessenberg matrices}.\\~\\
	\item		In particular, for $r=n+1$, we find
			$$
			Q_n^TAQ_n=H_n\in\mathbb{R}^{n\times n}.
			$$
			With other words, with the help of the Arnoldi iteration we can find in finitely many steps (at most $n$ steps) an upper Hessenberg $H_n$, which is orthogonally similar to $A$ and thus has the same eigenvalues. That is why one can tailor the $QR$-algorithm (which was an algorithm to compute eigenvalues) to matrices of Hessenberg structure. The Arnoldi iteration is therefore also considered an eigenvalue algorithm. Furthermore, if $A$ is symmetric, so is $H_n$, which then becomes a tridiagonal matrix.
\end{itemize}

\end{frame}

\begin{frame}
 \textbf{GMRES -- Step 2}
 ``Compute $QR$-decomposition $\widetilde{Q}_r\widetilde{R}_r:=AQ_r$''
 ~\\~\\
Again, we want to rely on the computations from the previous steps, i.e., $\widetilde{Q}_{r-1} = [\widetilde{q}_{1},\ldots,\widetilde{q}_{r-1}] \in \R^{n \times (r-1)}$ and $\widetilde{R}_{r-1} = [\widetilde{r}_{1},\ldots,\widetilde{r}_{r-1}]\in \R^{(r-1) \times(r-1)}$ with $   \widetilde{Q}_{r-1}\widetilde{R}_{r-1}={\color{cyan}AQ_{r-1} }$. Therefore, let us first observe that
\begin{align*}
\widetilde{Q}_r\widetilde{R}_r\stackrel{!}{=}AQ_r
=A\cdot[Q_{r-1}|q_r]=[{\color{cyan}AQ_{r-1} }|Aq_r]
=[\widetilde{Q}_{r-1}\widetilde{R}_{r-1}|Aq_r]  \in \R^{n \times r}.
\end{align*}
Let us use one Arnoldi step to orthogonalize $Aq_r$ against $\widetilde{q}_1,\dots,\widetilde{q}_{r-1}$ to obtain
%to obtain $\widetilde{q}_r$ and some numbers $(\widetilde{r}_{1,r},\dots,\widetilde{r}_{r,r})^\top := \widetilde{r}_r \in \R^r$, where $\widetilde{r}_{j,r}$, for $j\leq r$, correspond to the linear coefficient as they occur in the Arnoldi iteration (see the $h_{j,\cdot}$).
%~\\~\\ 
%More precisely. Let ~~
$$\widetilde{q}_r,\widetilde{r}_r :=  \texttt{Arnoldi\_step} (\widetilde{q}_1,\dots,\widetilde{q}_{r-1};Aq_r),$$
where the vector $\widetilde{q}_r$ and the coefficients of $\widetilde{r}_r = (\widetilde{r}_{1,r},\ldots, \widetilde{r}_{r,r})^\top \in \R^r$ are computed via
\begin{align*}
\widehat{q}_r&:=Aq_r-\sum_{j=1}^{r-1} \widetilde{r}_{j,r}\cdot \widetilde{q}_j,~~~~\widetilde{r}_{j,r} :=\widetilde{q}_j^\top(Aq_r) ,~~~~\widetilde{q}_r:=\frac{\widehat{q}_r}{\|\widehat{q}_r\|_2}, ~~~~\widetilde{r}_{rr}:=\|\widehat{q}_r\|_2,
\end{align*}
from which we can conclude
$$Aq_r =\widetilde{r}_{rr}\widetilde{q}_r+\sum_{j=1}^{r-1}\widetilde{r}_{j,r}\widetilde{q}_j=\sum_{j=1}^{r}\widetilde{r}_{j,r}\widetilde{q}_j.~~~\textbf{\color{orange}(*)}
$$
Then let us define potential candidates for the sought-after $QR$-decomposition as follows:
$$
\widetilde{Q}_r:=[\widetilde{Q}_{r-1} | \widetilde{q}_r],~~~
\widetilde{R}_r:=
\begin{pmatrix}
\widetilde{R}_{r-1}&|\\
---&\widetilde{r}_r\\
0\cdots 0 &|
\end{pmatrix}\in\mathbb{R}^{r\times r}.
$$
By \textbf{\color{orange}(*)} we can write $Aq_r  =\widetilde{Q}_r\widetilde{r}_r$ and indeed find, as desired,
$$~~AQ_r =({\color{cyan}AQ_{r-1} }|Aq_r)
=(\widetilde{Q}_{r-1}\widetilde{R}_{r-1}|\widetilde{Q}_r\widetilde{r}_r)=\widetilde{Q}_r\widetilde{R}_r.
 $$
\end{frame}

\begin{frame}
 \textbf{GMRES -- Step 3}
 ``Solve a triangular system''
 ~\\~\\
 The last step is easily performed by invoking a routine to solve the upper triangular system
 $$
 \widetilde{R}_rc_r=\widetilde{Q}_r^Tb$$ via backward substitution. Then we find our $r$-th iterate by setting
 $$x_r:=Q_rc_r.$$
\end{frame}




\begin{frame}
\Subsubsection{Summary and final Remarks: GMRES with Arnoldi}
		\begin{itemize}
			\item [i)]
			We only need one matrix-vector product $v\mapsto Av$ in each step! This product can be delivered to the solver \texttt{GMRES} as some sort of black box function.\\~\\
			\item [ii)] \textbf{An initial guess $x_0\neq 0$:}\\
			Consider $$x_r:=x_0+p_r,~~~\text{for some}~~x_0\neq 0,$$
			then
			$$Ax_r=b~~\Leftrightarrow~~A(x_0+p_r)=b	\Leftrightarrow~~Ap_r=b-Ax_0=\widehat{b}.$$
			Then we could solve the auxiliary system to obtain $p:=~\texttt{GMRES}(A,\widehat{b})$ and set
			$x:=x_0+p$. The above algorithm can therefore easily be extended to allow for an initial guess $x_0$ other than the zero vector. Also note that the Krylov subspaces are then generated based on the initial residual $\widehat{b}=b-Ax_0$.\\~\\
			\item [iii)]  \textbf{Preconditioning:} \\
			Let us consider $N\in \text{GL}_n(\mathbb{R}),~N\approx A^{-1}$, where $v\mapsto Nv$ is also easy to compute. Then
			$$
			Ax=b~~\Leftrightarrow~~\underbrace{NA}_{=:\widetilde{A}}x=\underbrace{Nb}_{=:\widetilde{b}}
			\Leftrightarrow~~\widetilde{A}x=\widetilde{b}.
			$$
			Then we could solve the equivalent system to obtain $x:=~\texttt{GMRES}(\widetilde{A},\widetilde{b})$.\\~\\
		    What is actually a good precondition?  Quite a bit of research has been put into this question over the last decades. What one can say: Krylov subspace methods converge quickly if the eigenvalues appear in clusters away from zero.
		\end{itemize}

\end{frame}

\begin{frame}
	~\\
	\begin{itemize}
		\item [iv)] \textbf{Restarted GMRES:}
		\begin{itemize}
			\item We need to store $Q_r:=(q_1,\dots,q_r),~\widetilde{Q}_r:=(\widetilde{q}_1,\dots,\widetilde{q}_r)$ and $\widetilde{R}_r:=(\widetilde{r}_1,\dots,\widetilde{r}_r)$. Thus, if the above algorithm runs many iterations, then $Q_r,\widetilde{Q}_r$ may become large (usually dense) matrices ($n\gg 1$), then the advantage of the sparsity of $A$ is lost.
			\item \underline{Idea:}	Perform only a fixed number of iterations, say $m=20-40$ and then restart \texttt{GMRES} with initial guess $x_m$. The resulting algorithm is sometimes coined \texttt{GMRES(m)}. You will later learn about a similar idea in the context of the L-BFGS method.
			\item \underline{Remark:} If $A$ is symmetric and positive definite, then one can show that we do not need the whole history $q_1,\dots,q_r$. This is the power of the CG-method which you will investigate next semester. 
		\end{itemize}
	~\\
		\item [v)] \textbf{Another perspective and variant:} Let us consider
		\begin{align*}
		\min_{x_r\in K_r(A,b)}\|Ax_r-b\|_2^2&=\min_{c_r\in\mathbb{R}^r}\|AQ_rc_r-b\|_2^2
		=\min_{c_r\in\mathbb{R}^r}\|Q_{r+1}H_{r+1,r}c_r-b\|_2^2\\
		&\stackrel{\textbf{\color{orange}(\#)}}{=}\min_{c_r\in\mathbb{R}^r}\underbrace{\|H_{r+1,r}c_r-\begin{pmatrix}
			\|b\|\\0\\\vdots\\0
			\end{pmatrix}\|_2^2}_{\text{we can evaluate this residual without computing}~x_r !}~~~\textbf{\color{cyan}(*)}\\
	 \textbf{\small\color{orange}(\#)}{\color{orange}:~q_1:=\frac{b}{\|b\|}~~}&{\color{orange}\Rightarrow~~b=q_1\|b\|~~\Rightarrow~~Q_{r+1}^Tb=\begin{pmatrix}
		\|b\|\\0\\\vdots\\0
		\end{pmatrix}~~\Rightarrow~~b=Q_{r+1}\begin{pmatrix}
		\|b\|\\0\\\vdots\\0
		\end{pmatrix} }
		\end{align*}
	\end{itemize}
\end{frame}

\begin{frame}
Let $\widehat{Q}_{r+1}\widehat{R}_r:=H_{r+1,r}$ be a $QR$-decomposition, then the normal equation associated to $\textbf{\color{cyan}(*)}$ could be solved by:
$$
\widehat{R}_rc_r=\widehat{Q}_{r+1}^T\begin{pmatrix}
\|b\|\\0\\\vdots\\0
\end{pmatrix}
$$
This results in a different variant of the GMRES and the following adaptions would apply:
\begin{itemize}
	\item \underline{In step 2:} We could derive such a $QR$-decomposition $\widehat{Q}_{r+1}\widehat{R}_r:=H_{r+1,r}$ from our arrays by setting $$\widehat{Q}_{r+1}:=Q_{r+1}^T\widetilde{Q}_r,~~\widehat{R}_r:=\widetilde{R}_r $$ because then
	\begin{align*}
	\widetilde{Q}_r\widetilde{R}_r\stackrel{!}{=}AQ_r=Q_{r+1}H_{r+1,r}
	\Leftrightarrow~~Q_{r+1}^T\widetilde{Q}_r\widetilde{R}_r=H_{r+1,r}
	\Leftrightarrow~~\widehat{Q}_{r+1}\widehat{R}_r = H_{r+1,r} ~~\textbf{\color{cyan}(*)}.
	\end{align*}
	~\\
	\item \underline{In step 3:} Our normal equation could then by written as
	\begin{align*}
	\widetilde{R}_rc_r=\widetilde{Q}_r^Tb~~\Leftrightarrow~~\widetilde{R}_rc_r=(Q_{r+1}^T\widetilde{Q}_r)^T\begin{pmatrix}
	\|b\|\\0\\\vdots\\0
	\end{pmatrix}
	\Leftrightarrow~~\widehat{R}_rc_r=\widehat{Q}_{r+1}\begin{pmatrix}
	\|b\|\\0\\\vdots\\0
	\end{pmatrix}
	\end{align*}
\end{itemize}
\end{frame}
\begin{frame}
	\small
	All in all we obtain the following algorithm:\\~\\
	\textbf{INPUT:} $A\in GL_n(\mathbb{R})$, $b \in \mathbb{R}^n$\\
	\textbf{OUTPUT:} approximation $x_r \in K_r(A,b)$ to the exact solution $A^{-1}b$\\~\\
	\texttt{\normalsize GMRES}($A$, $b$, $x_0 = 0$,\texttt{ tol = 1e-6}, \texttt{maxiter=None}, $N=I$ ):\\
\hspace*{1cm}\begin{minipage}{0.9\textwidth}
		$b = b - Ax_0$ {\color{gray}//account for initial guess}\\
		$A = NA$, $b = Nb$ {\color{gray}//account for preconditioner}\\
		{\color{gray}//Initialization:}\\
	$q_1 := \frac{b}{\|b\|_2}$,	$Q_1 := [q_1]$\\
	$v :=Aq_1,~~\tilde{q}_1 := \frac{v}{\|v\|_2}$,~~$\tilde{Q}_1 := [\tilde{q}_1]$,~~$\tilde{R}_1 = [\|v\|_2]$\\
	\For{$r = 2,...,\min(n,\texttt{maxiter})$}{
		{\color{gray}//STEP 1: use Arnoldi to find column $q_r$ by orthogonalizing $v$ against $q_1,\ldots, q_{r-1}$ } \\
		$q_r, h_{r-1} := \texttt{Arnoldi\_step}(Q_{r-1}; v)$ {\color{gray}//we don't need $h_{r-1}$ }\\
		$Q_r := [Q_{r-1},q_r]$\\
		$v:= Aq_r$\\~\\
		{\color{gray}//STEP 2: use Arnoldi to find columns $\tilde{q}_r,~\widetilde{r}_r$ by orthogonalizing $v$ against $\tilde{q}_1,\ldots, \tilde{q}_{r-1}$ } \\
		$\tilde{q}_r, \tilde{r}_r := \texttt{Arnoldi\_step}(\widetilde{Q}_{r-1}; v)$\\
		$\widetilde{Q}_r := [\widetilde{Q}_{r-1},\widetilde{q}_r]$, $\widetilde{R}_r := [\widetilde{R}_{r-1},\widetilde{r}_r]$\\~\\
		{\color{gray}//STEP 3: solve auxiliary least squares problems to obtain coordinates} \\
		$c_r := \texttt{solve\_triangular}(\widetilde{R}_r,\widetilde{Q}_r^\top b )$\\
		$x_r := Q_r c_r$	\\	
	{\color{gray}//Attention: Evaluate the original residual here:} \\
\If{$ \|N^{-1}(Ax_r - b)\|_2 < \texttt{tol}$}{
			break
		}		
	}
	\texttt{\textbf{return}} $x_0 + x_r$
\end{minipage}
\end{frame}